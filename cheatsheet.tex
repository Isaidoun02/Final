\documentclass[letterpaper,10pt]{article}
\usepackage[margin=0.4in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}

\title{\vspace{-4cm}Reinforcement Learning Algorithms Cheat Sheet}
\author{}
\date{}

\begin{document}
\maketitle
\normalsize
\begin{multicols}{2}

\section*{1. REINFORCE (On-Policy)}
\textbf{Summary:} (aka. Vanilla Policy Gradient)
\begin{itemize}
    \item Innovation: policy gradient method. Allows direct optimization of policies by calculating gradients of expected rewards
    \item Shift from value-based methods, simplifying learning in environments where VFA is challenging
    \item Good for episodic tasks; continuous or discrete action spaces.
    \item Uses Monte Carlo approach; update policy after each episode completes based on cumulative return/objective function. (Calculate gradient based on log likelihood of actions)
    \item A baseline can be introduced to reduce variance and stabilize learning
\end{itemize}

\noindent \textbf{Objective:}
\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
\]

\noindent \textbf{Policy Gradient:}
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\]
where \( G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k \) is the return from time \( t \).

\noindent \textbf{Policy Update:}
\[
\theta \leftarrow \theta + \alpha \cdot \nabla_\theta J(\theta)
\]

\noindent \textbf{Variance Reduction (Optional Baseline):}
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \left( G_t - b(s_t) \right) \right]
\]

\noindent \textbf{Networks Used:} 
\begin{itemize}
    \item \textbf{Policy Network} (Online)
\end{itemize}

\noindent \textbf{Hyperparameters: }Learning Rate (\(\alpha\)), Discount Factor (\(\gamma\))

\section*{2. DQN, DDQN, + Prioritized (Off-Policy)}
\textbf{DQN Summary:}
\begin{itemize}
    \item Combined Q-learning with deep neural networks to approximate the Q-value function for complex environments; scaling Q-learning to high-dimensional state spaces like Atari games
    \item Introduced experience replay and a target network to stabilize training and prevent the divergence of Q-values
    \item Appropriate for discrete action spaces with large state spaces
    \item Approximates Q-values using a neural network, updating the network parameters via backpropagation using a loss function based on the Bellman equation
\end{itemize}

\noindent \textbf{DDQN Summary:}
\begin{itemize}
    \item Fixes overestimation bias in DQN by decoupling the action selection and value evaluation steps. Improves stability and accuracy of Q-value estimates
    \begin{itemize}
        \item Innovation: Using two separate networks for selecting actions and another for evaluating Q-values
        \item More suitable for environments where Q-value estimates have high variance
    \end{itemize}
\end{itemize}

% \noindent \textbf{Prioritized Replay:} Prioritized Experience Replay DDQN enhances DDQN by focusing the learning process on more important experiences (i.e., experiences with high temporal difference (TD) error). This innovation makes learning more efficient by prioritizing the replay of transitions that are more informative. It is particularly effective in environments where some experiences contribute more significantly to learning than others. Prioritized replay selects experiences based on their TD error, leading to faster convergence and better performance.

\noindent \textbf{DQN Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma \max_{a'} Q_{\theta^-}(s', a') - Q_\theta(s, a) \right)^2 \right]
\]
"For the next state, we select the action with the max q-value from the target (offline) network, and evaluate it against the same target (offline) network"

\noindent \textbf{DDQN Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma Q_{\theta^-}(s', \text{argmax}_{a'} Q_\theta(s', a')) - Q_\theta(s, a) \right)^2 \right]
\]
"For the next state, we select the action with the max q-value from the Q-Network (online) network and evaluate it against the other target (offline) network"

\noindent \textbf{Prioritized Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \frac{p_i}{\text{Priority\_Sum}} \text{(Same as DQN/DDQN Loss Error)} \right]
\]
\textbf{Networks Used:} 
\begin{itemize}
    \item \textbf{Q-Network} (\(\theta\), Online)
    \item \textbf{Target Q-Network} (\(\theta^-\), Offline)
\end{itemize}
\textbf{Hyperparameters: }Learning Rate (\(\alpha\)), Discount Factor (\(\gamma\)), Batch Size, Target Network Update Frequency, Replay Buffer Size, Priority Exponent (\(\beta\)), Priority Scaling Factor (\(\epsilon\))


\section*{3. A2C/A3C (On-Policy)}
\textbf{A2C Summary:}
\begin{itemize}
    \item Innovation: Combine actor-critic architecture with advantage function to reduce variance in policy gradient updates
    \begin{itemize}
        \item Provides more stable learning signal; improves the efficiency and stability of policy learning
    \end{itemize}
    \item Appropriate for environments requiring stable policy optimization
    \item Uses value function (critic) to estimate the advantage, which is then used to update the policy (actor)
\end{itemize}

\noindent \textbf{Asynchronous Advantage Actor-Critic (A3C) Summary:}
\begin{itemize}
    \item Extends A2C: runs multiple agents in parallel, each exploring different parts of the environment
    \item Improvement: Inefficiencies of single-threaded A2C, so good for complex environments where parallel exploration speeds up the learning process. Agents update a global model asynchronously
\end{itemize}

\noindent \textbf{Actor Objective:}
\[
\nabla_\theta J(\theta) = \mathbb{E}_{s_t, a_t} \left[ \nabla_\theta \log \pi_\theta(a_t | s_t) A(s_t, a_t) \right]
\]
\text{Actor is updated using gradient ascent using the objective.}
\noindent \textbf{Critic Loss:}
\[
L(\phi) = \mathbb{E}_{s_t} \left[ \left( R_t - V_\phi(s_t) \right)^2 \right]
\]
\text{Critic is updated using stochastic gradient descent using the loss.}
\[
\phi = \phi - \alpha \nabla_\phi L(\phi)^2
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Actor Network} (\(\theta\), Online)
    \item \textbf{Critic Network} (\(\phi\), Online)
    \item \textbf{Multiple Workers (A3C)}
\end{itemize}
\textbf{Hyperparameters: }Actor Learning Rate (\(\alpha\)), Critic Learning Rate (\(\beta\)), Discount Factor (\(\gamma\)), Number of Workers (A3C)

\section*{4. DPG, DDPG (Off-Policy Actor-Critic Methods)}
\textbf{DPG Summary:}
\begin{itemize}
    \item Innovation: Use deterministic policies instead of stochastic ones; allows algorithm to directly optimize the expected return with respect to deterministic actions
    \item Useful in continuous action spaces where stochastic policies may be inefficient
    \item Operates with one deterministic policy and a critic that evaluates the actions chosen by this policy.
\end{itemize}

\noindent \textbf{DDPG Summary:}
\begin{itemize}
    \item Extends DPG: Uses deep neural networks for function approximation, experience replay, and target networks
    \item Scales to high-dimensional state and action spaces
\end{itemize}

\noindent \textbf{DPG/DDPG Actor Update:}
\[
\nabla_\phi J(\phi) = \mathbb{E}_s \left[ \nabla_a Q_\theta(s, a) \big|_{a=\mu_\phi(s)} \nabla_\phi \mu_\phi(s) \right]
\]
\textbf{DDPG Critic Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma Q_{\theta^-}(s', \mu_{\phi^-}(s')) - Q_\theta(s, a) \right)^2 \right]
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Actor Network} (\(\phi\), Online)
    \item \textbf{Critic Network} (\(\theta\), Online)
    \item \textbf{Target Actor Network} (\(\phi^-\), Offline)
    \item \textbf{Target Critic Network} (\(\theta^-\), Offline)
\end{itemize}
\textbf{Hyperparameters: }Actor Learning Rate (\(\alpha\)), Critic Learning Rate (\(\beta\)), Discount Factor (\(\gamma\)), Replay Buffer Size, Batch Size, Soft Update Rate (\(\tau\)), Target Policy Noise


\section*{5. TRPO, PPO (On-Policy)}
\textbf{TRPO (Trust Region Policy Optimization) Summary:}
\begin{itemize}
    \item Innovation: Hard constraint on the policy update to ensure it stays within a trust region; prevents large, destabilizing updates
    \item Adresses maintaining stability in policy learning while still making meaningful progress
    \item Good for complex environments where policy stability is critical
    \item Ensures updates do not deviate too far from the previous policy
\end{itemize}

\noindent \textbf{PPO (Proximal Policy Optimization) Summary:}
\begin{itemize}
    \item Improves TRPO's trust region approach by using a clipped objective function instead of a hard constraint
    \begin{itemize}
        \item Makes the algorithm easier to implement and tune; more practical and scalable
        \item Instead of ignoring updates outside trust region, clips them to be within a certain range (max or min, not ignored)
    \end{itemize}
    \item Good for discrete and continuous action spaces   
    \item Balances exploration and exploitation with the clipped objective function 
\end{itemize}

\noindent \textbf{TRPO Objective:}
\[
\max_\theta \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\theta_{\text{old}}}}(s, a) \right]
\]
\textbf{PPO Objective:}
\[
\max_\theta \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
\]
\[
r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Policy Network} (Online)
    \item \textbf{Value Network} (Online)
\end{itemize}
\textbf{Hyperparameters: }Learning Rate (\(\alpha\)), Discount Factor (\(\gamma\)), KL Divergence Constraint (\(\delta\)) (TRPO only), Clip Range (\(\epsilon\)) (PPO only), Number of Epochs, Batch Size

\section*{6. MCTS (Off-Policy)}
\textbf{Summary:}
\begin{itemize}
    \item Innovation: Combine tree search methods with Monte Carlo simulations to explore future states and make decisions based on empirical outcomes
    \item Addresses decision-making in complex environments by systematically exploring and evaluating potential outcomes
    \item Suitable for games and decision-making problems with complex state spaces
    \item Constructs a search tree and follows: Selection, Expansion, Simulation, Backpropagation
\end{itemize}

\noindent \textbf{UCT (Upper Confident Trees) Formula:}
\[
Q(s, a) = \frac{w(s, a)}{n(s, a)} + c \sqrt{\frac{\ln N(s)}{n(s, a)}}
\]

\noindent \textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Search Tree} (Online)
\end{itemize}

\noindent \textbf{Hyperparameters: }Exploration Constant (\(c\)): Empirical, Number of Simulations, Tree Depth: Problem-specific


\section*{7. AlphaGo, AlphaZero (Off-Policy)}
\textbf{AlphaGo Summary:}
\begin{itemize}
    \item Innovation: Uses MCTS paired with deep neural networks to enhance MCTS
    \item Effective for board games with very large state spaces
\end{itemize}

\noindent \textbf{AlphaZero Summary:}
\begin{itemize}
    \item Innovation: Extends AlphaGo by combining MCTS with deep neural networks trained entirely through self-play (generate its own data to train itself)
\end{itemize}

\noindent \textbf{Loss Function:}
\[
L(\theta) = \mathbb{E}_{(s, \pi, z) \sim D} \left[ (z - V(s; \theta))^2 - \pi \cdot \log \pi(a|s; \theta) + \lambda \|\theta\|^2 \right]
\]

\noindent \textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Policy Network} (Online)
    \item \textbf{Value Network} (Online)
\end{itemize}

\noindent \textbf{Hyperparameters: }Learning Rate (\(\alpha\)), Discount Factor (\(\gamma\)), MCTS Simulations, Temperature: Annealed over time, Regularization Term (\(\lambda\))

% Additional Notes Sections
\section*{8. Advantage Function}
\textbf{Summary:} The Advantage function \( A(s, a) \) estimates how much better it is to take a specific action \( a \) in state \( s \), compared to the average action in that state. It can be defined as:
\[
A(s, a) = Q(s, a) - V(s)
\]
The advantage function helps in reducing variance and is often used in Actor-Critic methods to stabilize learning.

\section*{9. Stochastic Gradient Descent}
\textbf{Summary:} SGD is a method used to minimize an error function (like loss or TD-error) \( \nabla_w Error^2 \) through backpropagation. The update rule for SGD is:
\[
w \leftarrow w - \alpha \nabla_w Error^2
\]
Where \( \alpha \) is the learning rate. The objective is to find the local minima of the error function by adjusting the weights in the direction of the steepest descent.

\noindent \textbf{Ascent Variation:} Gradient ascent is used to maximize a function, such as an objective or reward function. Instead of minimizing the error, gradient ascent increases the return or reward by updating the weights in the direction of the steepest ascent:
\[
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
\]
This technique is commonly used in reinforcement learning to optimize policy parameters.

\section*{10. Value Function Approximation (VFA)}
\textbf{Summary:} VFA is a technique used to represent value functions in environments with large state and action spaces, where tabular methods become impractical. By using function approximation, we can generalize across states or state-action pairs; \( \hat{v}(s,w) \) or \( \hat{q}(s,a,w) \), where the parameter \(w \) is updated using learning methods. This reduces memory and computational requirements.

\noindent \textbf{Function Approximation Methods:}
\begin{itemize}
    \item \textbf{Linear Function Approximation:} The value function is represented as a linear combination (dot product) of features:
    \[
    \hat{v}(s, w) = w^T x(s) = \sum_{i=1}^{n} w_i \cdot x_i(s)
    \]
    where \(w\) is the weight vector, and \(x(s)\) is the feature vector (some property or aspect of state \( s \)).
    \item \textbf{Common Approximators:} Linear functions, Neural Networks, Decision Trees, Nearest Neighbors, and Fourier/Wavelet Bases.
\end{itemize}

VFA can be used in both Monte Carlo and Temporal Difference (TD) methods. The function approximator helps in learning the value function incrementally, making the learning process more efficient in large state-action spaces.


\section*{11. Neural Networks (CNN, RNN, Regularization, Overfitting, Pooling)}
Convolutional Neural Network (CNN) are commonly used and use convolutions instead of matrix multiplications to exploit spatial structure in data. They are not fully connected like a standard neural network to save on computation.

\noindent Recurrent Neural Networks (RNN) are designed to handle sequential data by maintaining a hidden state that captures temporal dependencies.

\noindent Regularization techniques like L1 and L2 regularization prevent overfitting by adding a penalty term to the loss function. Overfitting occurs when a model learns the training data too well, leading to poor generalization.

\noindent \textbf{Usage}
\begin{itemize}
    \item L1: Add \( \lambda |w| \) to each weight error function (encourages all weights to zero)
    \item L2 (Weight Decay): Augment the error function with \( \frac{1}{2} \lambda w^2 \) encourages small weights/to use all inputs instead of relying on strongest signal
\end{itemize}
Pooling is used in CNNs to reduce the spatial dimensions of the input and improve invariance, for example we combine/pool nearby neurons to reduce the size of the representation, like taking the max or average of a group of neurons.

\section*{12. Objective Function}
\textbf{Summary:} The objective function \( J(\theta) \) represents the expected cumulative return. The goal is to find parameters \( \theta \) that maximize it. It is often defined as:
\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)] = \sum_{\tau} P(\tau;\theta)R(\tau)
\]
Where \( \tau \) represents the trajectory, which is the sequence of states and actions. The cumulative return \( R(\tau) \) is taken over all possible trajectories, and \( P(\tau;\theta) \) represents the probability of the possible trajectory under policy \( \pi_\theta \).

\section*{13. Experience Replay \& Prioritized Experience Replay}
\textbf{Experience Replay:} This technique stores the agent's experiences in a replay buffer, allowing the agent to learn from a (uniformly) randomly sampled subset of past experiences to break correlations between consecutive samples.
\[
e_t = (s_t, a_t, r_{t+1}, s_{t+1})
\]
\textbf{Prioritized Experience Replay:} This variant assigns higher sampling probability to experiences with larger Temporal Difference (TD) errors, making learning more efficient by focusing on more informative experiences. The idea is that not all experiences are equally valuable. Some experiences (like those involving significant errors in prediction) can provide more informative updates to the agent's policy.

\section*{14. REINFORCE Baseline}
\textbf{Summary:} The baseline \( b(s_t) \) is subtracted from the return \( R(\tau) \) to reduce the variance of the gradient estimates. This makes the learning process more stable and efficient. The policy gradient with baseline is:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t | s_t) (R(\tau) - b(s_t)) \right]
\]
Adding a baseline does not introduce any bias into the estimates of the policy gradient since it does not affect the expectation of the gradient estimates because it is usually chosen to be independent of the action taken

\section*{15. Soft Update DQN (\(\tau\))}
\textbf{Summary:} In DQN, a soft update is used to gradually update the target network parameters \( \theta^- \) using the online network parameters \( \theta \):
\[
\theta^- \leftarrow \tau \theta + (1 - \tau) \theta^-
\]
Where \( \tau \) is a small number, typically \( \tau = 0.005 \), ensuring smooth updates and preventing instability during training. This differs from a hard update, where the target network is updated directly with the online network parameters. \( \theta^- \leftarrow \theta \)

\section*{16. Policy Gradient Theorem}
\textbf{Summary:} The Policy Gradient Theorem provides a way to maximize the objective function \( J(\theta) \) by \textbf{estimating the gradient} \( \nabla_\theta J(\theta) \) as \( \hat{g} \)
\[
\nabla_\theta J(\theta) \approx \hat{g} = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
\]
\textbf{Key Points:}
\begin{itemize}
    \item The gradient \( \nabla_\theta \log \pi_\theta(a_t | s_t) \) represents the direction of the steepest increase in the log probability of selecting action \( a_t \) at state \( s_t \).
    \item This gradient tells us how to adjust the policy weights to increase or decrease the log probability of choosing action \( a_t \) at state \( s_t \).
    \item The reward \( R(\tau) \) influences the update directionâ€”high rewards increase the log probability of the (state, action) combination, while low rewards decrease it.
\end{itemize}
\textbf{Policy Update:}
\[
\theta \leftarrow \theta + \alpha \hat{g}
\]
\( \hat{g} \) serves as an approximation of the true policy gradient \( \nabla_\theta J(\theta) \), which is computationally expensive because of calculating the probability of each possible trajectory. \( \hat{g} \) is computed using sampled trajectories.

\section*{17. Importance Sampling}
A technique used to estimate the properties of a particular distribution, while only having samples generated from a different distribution than the one of interest. This approach can greatly improve the efficiency of Monte Carlo simulations by reducing variance, especially when dealing with high-dimensional data.

\section*{18. Bootstrapping}
Bootstrapping methods mean that it updates the value estimates based on other estimated values, rather than waiting for the final outcome (trajectory) of the episode.

\section*{19. Bellman Equation}
\[
v_\pi(s) = \sum_{a \in A} \pi(a | s) \left[ \sum_{s' \in S} p(s' | s, a) [r + \gamma v_\pi(s')]\right]
\]
Probability we take specific action \( a \) in state \( s \) (policy) for all actions.
Probability of we actually end up in state \( s' \) if we take action \( a \) in state \( s \) (deterministic or not).

\section*{20. Policy Improvement Theorem}
We can always greedify to obtain a better policy \(\pi'\), where "better" is \(q_{\pi'}(s,a) \geq q_\pi(s,a)\) for all \(s,a\). Basically keep greedifying until \(\pi\) is the same as \(\pi'\):
\[
\pi'(s) = \arg \max_a q_\pi(s, a)
\]

\section*{21. Eligibility Traces}
\textbf{Summary:} Eligibility traces are a mechanism used to bridge the gap between Monte Carlo methods (which use complete returns) and Temporal Difference (TD) methods (which use one-step returns). They allow for a mix between these two extremes, enabling more efficient learning by considering a broader range of rewards over multiple steps.

\begin{itemize}
    \item In Monte Carlo, every state-action pair that occurs in an episode is updated based on the complete return.
    \begin{itemize}
        \item Every state gets \( \frac{r}{t} \) reward (equal) for each time step \( t \) in the episode. Equivalent to TD(\( \lambda=1 \))
    \end{itemize}
    \item In TD(\( \lambda=0 \)), when terminal state reached, only the immediate state-action pair that led to the terminal state is updated with the reward.
    \item In TD(\( \lambda=0.9 \)), the immediate state-action pair that led to the terminal state gets the full reward, but previous state-action pairs get decayed reward (scaled by \( \lambda \)).
    \item Also has a threshold for the minimum eligibility trace value, below which the trace (and reward) is set to zero.
\end{itemize}

\section*{22. N-Step Prediction}
\textbf{Summary:} Used in TD learning methods, to update the value of a state by looking ahead multiple steps in the future.

\begin{itemize}
    \item 1-step equivalent to TD(0), need to only look ahead one step to update the value of a state.
    \item \( \infty \)-step is equivalent to Monte Carlo, need to look ahead to the end of the episode to update the value of a state.
\end{itemize}

\section*{23. SARSA: On-Policy TD Control}
\begin{itemize}
    \item Initialize \( Q(s,a) \) for all \( s \in S \), \( a \in A(s) \) arbitrarily, and \( Q(\text{terminal-state}, \cdot) = 0 \)
    \item Repeat (for each episode):
    \begin{itemize}
        \item Initialize \( S \)
        \item Choose \( A \) from \( S \) using policy derived from \( Q \) (e.g., \(\epsilon\)-greedy)
        \item Repeat (for each step of episode):
        \begin{itemize}
            \item Take action \( A \), observe \( R, S' \)
            \item Choose \( A' \) from \( S' \) using policy derived from \( Q \) (e.g., \(\epsilon\)-greedy)
            \item \( Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma Q(S', A') - Q(S,A)] \)
            \item \( S \leftarrow S' \); \( A \leftarrow A' \)
        \end{itemize}
        \item until \( S \) is terminal
    \end{itemize}
\end{itemize}

\section*{24. Expected SARSA: On-Policy TD Control}
\[
Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + 
\gamma \sum_a \pi(a \mid S') Q(S', a) - Q(S, A) \right]
\]
\begin{itemize}
    \item \( \pi(a|s) = 0.9\) for max \(a\), \(\frac{0.1}{|A|-1}\) for others
    \item SARSA \& Q-Learning, vulnerable to sample bias; where rewards are sparse or highly variable causes few observed rewards to not accurately represent the overall reward distribution
\end{itemize}

\section*{25. Q-Learning: Off-Policy TD Control}
\begin{itemize}
    \item Initialize \( Q(s,a) \) for all \( s \in S \), \( a \in A(s) \) arbitrarily, and \( Q(\text{terminal-state}, \cdot) = 0 \)
    \item Repeat (for each episode):
    \begin{itemize}
        \item Initialize \( S \)
        \item Repeat (for each step of episode):
        \begin{itemize}
            \item Choose \( A \) from \( S \) using policy derived from \( Q \) (e.g., \(\epsilon\)-greedy)
            \item Take action \( A \), observe \( R, S' \)
            \item \( Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma \max_{a} Q(S', a) - Q(S,A)] \)
            \item \( S \leftarrow S' \)
        \end{itemize}
        \item until \( S \) is terminal
    \end{itemize}
\end{itemize}

\section*{26. Double Q-Learning}
\begin{itemize}
    \item Initialize \( Q_1(s,a) \) and \( Q_2(s,a) \) for all \( s \in S \), \( a \in A(s) \) arbitrarily
    \item Initialize \( Q_1(\text{terminal-state}, \cdot) = Q_2(\text{terminal-state}, \cdot) = 0 \)
    \item Repeat (for each episode):
    \begin{itemize}
        \item Initialize \( S \)
        \item Repeat (for each step of episode):
        \begin{itemize}
            \item Choose \( A \) from \( S \) using policy derived from \( Q_1 \) and \( Q_2 \) (e.g., \(\epsilon\)-greedy in \( Q_1 + Q_2 \))
            \item Take action \( A \), observe \( R, S' \)
            \item With 0.5 probability:
            \begin{itemize}
                \item \( Q_1(S,A) \leftarrow Q_1(S,A) + \alpha [R + \gamma Q_2(S', \arg \max_{a} Q_1(S',a)) - Q_1(S,A)] \)
            \end{itemize}
            \item else:
            \begin{itemize}
                \item \( Q_2(S,A) \leftarrow Q_2(S,A) + \alpha [R + \gamma Q_1(S', \arg \max_{a} Q_2(S',a)) - Q_2(S,A)] \)
            \end{itemize}
            \item \( S \leftarrow S' \)
        \end{itemize}
        \item until \( S \) is terminal
    \end{itemize}
\end{itemize}

\section*{27. Optimism Bias}
\textbf{Summary:} A systematic overestimation of the value of an action that occurs in Q-learning and other algorithms based on it like DQN. Happens because the state-action value estimates are updated using a greedy policy both for selecting the action to take, and for evaluating it's benefit in future states. Double Q-Learning decouples these two greedy selections by using two seperate value functions.

\section*{28. Other}
If policy gradient methods are off-policy, just like Q-learning,then why are they better able to find stable solutions quickly?

\begin{itemize}
    \item Off-policy \textbf{*value based*} methods like Q-learning, only focus on evaluating the policy locally, and hoping that this will allow the agent to choose the best action
    \item PG methods improve a policy directly towards an estimate of the optimal direction.
\end{itemize}

\end{multicols}
\end{document}