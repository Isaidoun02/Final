\documentclass[a4paper,10pt]{article}
\usepackage[margin=0.4in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}

\title{\vspace{-4cm}Reinforcement Learning Algorithms Cheat Sheet}
\author{}
\date{}

\begin{document}
\maketitle
\footnotesize
\begin{multicols}{2}

\section*{1. REINFORCE}
\textbf{Summary:} REINFORCE introduces the fundamental innovation of the policy gradient method, allowing direct optimization of policies by calculating gradients of expected rewards. This approach shifts from value-based methods, simplifying learning in environments where value function approximation is challenging. REINFORCE is well-suited for episodic tasks, particularly those with continuous or discrete action spaces. The algorithm uses a Monte Carlo approach, updating the policy after each episode based on cumulative return, with the gradient computed via the log-likelihood of actions. A baseline can be introduced to reduce variance and stabilize learning.

\textbf{Objective:}
\[
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
\]

\textbf{Policy Gradient:}
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\]
where \( G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k \) is the return from time \( t \).

\textbf{Policy Update:}
\[
\theta \leftarrow \theta + \alpha \cdot \nabla_\theta J(\theta)
\]

\textbf{Variance Reduction (Optional Baseline):}
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \left( G_t - b(s_t) \right) \right]
\]

\textbf{Networks Used:} 
\begin{itemize}
    \item \textbf{Policy Network} (Online)
\end{itemize}

\textbf{Hyperparameters:}
\begin{itemize}
    \item Learning Rate (\(\alpha\)): \(1e^{-3}\) to \(1e^{-4}\)
    \item Discount Factor (\(\gamma\)): 0.99
\end{itemize}

\section*{2. DQN, DDQN, Prioritized Replay DDQN}
\textbf{DQN Summary:} Deep Q-Network (DQN) was a fundamental innovation that combined Q-learning with deep neural networks to approximate the Q-value function for complex environments. It introduced experience replay and a target network to stabilize training and prevent the divergence of Q-values. DQN was developed to address the challenge of scaling Q-learning to high-dimensional state spaces, such as those found in Atari games. It is appropriate for discrete action spaces with large state spaces. DQN deals with states and actions by approximating the Q-values using a neural network, updating the network parameters via backpropagation using a loss function based on the Bellman equation.

\textbf{DDQN Summary:} Double DQN (DDQN) addresses the overestimation bias present in DQN by decoupling the action selection and value evaluation steps, thereby improving the stability and accuracy of Q-value estimates. The fundamental innovation in DDQN was to reduce the overestimation of Q-values by using two separate networks for selecting actions and another for evaluating the Q-values. This modification makes DDQN more suitable for environments where Q-value estimates have high variance. The update rule in DDQN: For the next state, we select the action with the max q-value from the Q-Network (online) network and evaluate it against the other target (offline) network

\textbf{Prioritized DDQN Summary:} Prioritized Experience Replay DDQN enhances DDQN by focusing the learning process on more important experiences (i.e., experiences with high temporal difference (TD) error). This innovation makes learning more efficient by prioritizing the replay of transitions that are more informative. It is particularly effective in environments where some experiences contribute more significantly to learning than others. Prioritized replay selects experiences based on their TD error, leading to faster convergence and better performance.

\textbf{DQN Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma \max_{a'} Q_{\theta^-}(s', a') - Q_\theta(s, a) \right)^2 \right]
\]
\textbf{DDQN Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma Q_{\theta^-}(s', \text{argmax}_{a'} Q_\theta(s', a')) - Q_\theta(s, a) \right)^2 \right]
\]
\textbf{Prioritized Replay DDQN Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \frac{p_i}{\text{Priority\_Sum}} \text{(Same as DDQN Loss Error)} \right]
\]
\textbf{Networks Used:} 
\begin{itemize}
    \item \textbf{Q-Network} (\(\theta\), Online)
    \item \textbf{Target Q-Network} (\(\theta^-\), Offline)
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Learning Rate (\(\alpha\)): \(1e^{-4}\) to \(1e^{-3}\)
    \item Discount Factor (\(\gamma\)): 0.99
    \item Batch Size: 32 to 64
    \item Target Network Update Frequency: 1000
    \item Replay Buffer Size: \(1e^5\) to \(1e^6\)
    \item Priority Exponent (\(\beta\)): 0.4 to 1.0
    \item Priority Scaling Factor (\(\epsilon\)): \(1e^{-5}\)
\end{itemize}

\section*{3. A2C/A3C}
\textbf{A2C Summary:} Advantage Actor-Critic (A2C) innovates by using the advantage function to reduce variance in policy gradient updates and combines actor-critic methods for more stable learning. This approach addresses the need for stability and efficiency in policy learning. A2C is appropriate for environments requiring stable policy optimization. It handles states and actions with a value function (critic) to estimate advantages, which helps update the policy (actor). The advantage function is \( \text{Adv}(s, a) = Q(s, a) - V(s) \), and the policy update is \( \theta \leftarrow \theta + \alpha \cdot \nabla_\theta \log \pi(a_t | s_t; \theta) \cdot \text{Adv}(s_t, a_t) \).
\textbf{A3C Summary:} Asynchronous Actor-Critic Agents (A3C) use multiple parallel agents to explore different parts of the environment and update a global model asynchronously. This innovation speeds up learning and enhances robustness by utilizing parallelism. A3C addresses the problem of slow and unstable learning by combining experiences from multiple agents. It is appropriate for environments where parallel computation can be leveraged. States and actions are managed by multiple agents exploring the environment, and rewards are used to update a shared global model asynchronously. The objective function is similar to A2C but includes asynchronous updates.
\textbf{Actor Update:}
\[
\max_\theta \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\theta_{\text{old}}}}(s, a) \right]
\]
\textbf{Critic Update:}
\[
L(\phi) = \mathbb{E}_{s_t} \left[ \left( R_t - V_\phi(s_t) \right)^2 \right]
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Actor Network} (Online)
    \item \textbf{Critic Network} (Online)
    \item \textbf{Multiple Workers (A3C)}
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Actor Learning Rate (\(\alpha\)): \(1e^{-4}\)
    \item Critic Learning Rate (\(\beta\)): \(1e^{-3}\)
    \item Discount Factor (\(\gamma\)): 0.99
    \item Number of Workers (A3C): 16 to 32
\end{itemize}

\section*{4. DPG, DDPG}
\textbf{DPG Summary:} Deterministic Policy Gradient (DPG) introduces the innovation of computing the gradient of expected rewards with respect to deterministic policy parameters, specifically for continuous action spaces. This method addresses the need for effective handling of continuous action spaces where traditional methods are not suitable. DPG is appropriate for continuous action spaces with deterministic policies. It handles states and actions by optimizing a deterministic policy and computes gradients directly. The policy gradient is \( \nabla_\theta J(\theta) = \mathbb{E}_{s \sim \rho_\pi} \left[ \nabla_\theta \pi(a|s; \theta) \cdot \nabla_a Q(s, a; \theta) \right] \).
\textbf{DDPG Summary:} Deep Deterministic Policy Gradient (DDPG) extends DPG by using deep neural networks for approximating both the policy and value functions, incorporating experience replay and target networks. This approach addresses the challenge of scaling to high-dimensional continuous action spaces. DDPG is suitable for high-dimensional continuous action spaces like robotic control. It manages states, actions, and rewards using deep networks for approximation and employs experience replay for training stability. The Q-value update is \( Q(s_t, a_t; \theta^Q) \leftarrow r_t + \gamma Q(s_{t+1}, \mu(s_{t+1}; \theta^\mu); \theta^Q) \), and the policy update is \( \nabla_\theta^\mu J = \mathbb{E} \left[ \nabla_a Q(s, a; \theta^Q) \nabla_\theta^\mu \mu(s; \theta^\mu) \right] \).
\textbf{DPG/DDPG Actor Update:}
\[
\nabla_\phi J(\phi) = \mathbb{E}_s \left[ \nabla_a Q_\theta(s, a) \big|_{a=\mu_\phi(s)} \nabla_\phi \mu_\phi(s) \right]
\]
\textbf{DDPG Critic Loss:}
\[
L(\theta) = \mathbb{E}_{(s, a, r, s')} \left[ \left( r + \gamma Q_{\theta^-}(s', \mu_{\phi^-}(s')) - Q_\theta(s, a) \right)^2 \right]
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Actor Network} (Online)
    \item \textbf{Critic Network} (Online)
    \item \textbf{Target Actor Network} (Offline)
    \item \textbf{Target Critic Network} (Offline)
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Actor Learning Rate (\(\alpha\)): \(1e^{-4}\) to \(1e^{-3}\)
    \item Critic Learning Rate (\(\beta\)): \(1e^{-3}\)
    \item Discount Factor (\(\gamma\)): 0.99
    \item Replay Buffer Size: \(1e^5\) to \(1e^6\)
    \item Batch Size: 100
    \item Soft Update Rate (\(\tau\)): 0.005
    \item Target Policy Noise: 0.2
\end{itemize}

\section*{5. TRPO, PPO}
\textbf{TRPO Summary:} Trust Region Policy Optimization (TRPO) innovates by optimizing policies within a trust region to prevent large, destabilizing policy updates. This method solves the problem of maintaining policy stability during updates. TRPO is appropriate for complex environments where policy stability is crucial. It handles states and actions through constrained optimization, ensuring that policy changes remain within a trust region. The objective is to maximize \( \mathbb{E}_{s_t} \left[ \text{Adv}(s_t, a_t) \right] \), subject to a constraint on the Kullback-Leibler (KL) divergence: \( \mathbb{E}_{s_t} \left[ \text{KL}(\pi_{\text{old}}(a|s) \| \pi(a|s)) \right] \leq \delta \).
\textbf{PPO Summary:} Proximal Policy Optimization (PPO) simplifies TRPO’s trust region optimization by using a clipped surrogate objective function to ensure stable policy updates. This innovation provides a practical and scalable approach to policy optimization. PPO solves the problem of maintaining stability in policy updates with a more straightforward implementation. It is suitable for both discrete and continuous action spaces. PPO manages states, actions, and rewards through a clipped objective function, balancing exploration and exploitation. The objective is to maximize \( \mathbb{E} \left[ \min \left( r_t(\theta) \cdot \text{Adv}(s_t, a_t), \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \cdot \text{Adv}(s_t, a_t) \right) \right] \), where \( r_t(\theta) = \frac{\pi(a_t | s_t; \theta)}{\pi(a_t | s_t; \theta_{\text{old}})} \).
\textbf{TRPO Objective:}
\[
\max_\theta \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A^{\pi_{\theta_{\text{old}}}}(s, a) \right]
\]
\textbf{PPO Objective:}
\[
\max_\theta \mathbb{E}_{s, a \sim \pi_{\theta_{\text{old}}}} \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
\]
\[
r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Policy Network} (Online)
    \item \textbf{Value Network} (Online)
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Learning Rate (\(\alpha\)): \(1e^{-4}\) to \(3e^{-4}\)
    \item Discount Factor (\(\gamma\)): 0.99
    \item KL Divergence Constraint (\(\delta\)): 0.01 to 0.02
    \item Clip Range (\(\epsilon\)): 0.1 to 0.2
    \item Number of Epochs: 3 to 10
    \item Batch Size: 64 to 2048
\end{itemize}

\section*{6. MCTS}
\textbf{Summary:} Monte Carlo Tree Search (MCTS) innovates by utilizing simulations to explore potential future states and build a search tree based on empirical outcomes. This method addresses decision-making in complex environments by exploring and evaluating future possibilities. MCTS is appropriate for games and decision-making problems with large state spaces. It manages states and actions through a search tree and evaluates possible outcomes using simulations. The objective involves balancing exploration and exploitation with the Upper Confidence Bound (UCB1) formula: \( \text{UCB1} = \bar{X}_j + C \sqrt{\frac{\ln N_i}{n_j}} \).
\textbf{UCT Formula:}
\[
Q(s, a) = \frac{w(s, a)}{n(s, a)} + c \sqrt{\frac{\ln N(s)}{n(s, a)}}
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Search Tree} (Online)
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Exploration Constant (\(c\)): Empirical
    \item Number of Simulations: 1000 to 10000
    \item Tree Depth: Problem-specific
\end{itemize}

\section*{7. AlphaGo, AlphaZero}
\textbf{AlphaGo Summary:} AlphaGo combines MCTS with deep neural networks for state evaluation and move selection, enabling it to tackle the complex game of Go. This innovation leverages deep learning to guide and improve MCTS. AlphaGo solves the problem of mastering Go, a game with immense complexity and strategic depth. It is appropriate for board games with large state spaces. States and actions are evaluated using neural networks, and MCTS is used for decision-making. The policy network outputs move probabilities, and the value network estimates winning chances from a given board state.
\textbf{AlphaZero Summary:} AlphaZero generalizes the AlphaGo approach by combining MCTS with deep learning trained through self-play, making it applicable to multiple board games. This innovation creates a versatile system capable of mastering various games without domain-specific knowledge. AlphaZero addresses the need for a universal learning system that can generalize across different games. It is suitable for board games and other decision-making problems. AlphaZero uses self-play to generate training data and combines it with neural networks and MCTS for decision-making. The objective involves optimizing both policy and value networks through self-play and MCTS.
\textbf{Loss Function:}
\[
L(\theta) = \mathbb{E}_{(s, \pi, z) \sim D} \left[ (z - V(s; \theta))^2 - \pi \cdot \log \pi(a|s; \theta) + \lambda \|\theta\|^2 \right]
\]
\textbf{Networks Used:}
\begin{itemize}
    \item \textbf{Policy Network} (Online)
    \item \textbf{Value Network} (Online)
\end{itemize}
\textbf{Hyperparameters:}
\begin{itemize}
    \item Learning Rate (\(\alpha\)): \(1e^{-3}\) to \(1e^{-2}\)
    \item Discount Factor (\(\gamma\)): 1.0
    \item MCTS Simulations: Several thousand
    \item Temperature: Annealed over time
    \item Regularization Term (\(\lambda\)): \(1e^{-4}\) to \(1e^{-3}\)
\end{itemize}

\end{multicols}
\end{document}
